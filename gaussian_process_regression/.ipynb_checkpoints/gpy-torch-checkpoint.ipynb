{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 2]) torch.Size([20]) torch.Size([10, 2]) torch.Size([10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'torch.DoubleTensor'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = r\"C:\\Users\\mikep\\Desktop\\project\\data\\final\"\n",
    "\n",
    "Xtrain = torch.from_numpy(np.genfromtxt(path+\"\\Xtrain.csv\", delimiter=\",\", skip_header=1))\n",
    "Ytrain = torch.from_numpy(np.genfromtxt(path+\"\\Ytrain.csv\", delimiter=\",\")).t()\n",
    "\n",
    "Xtest = torch.from_numpy(np.genfromtxt(path+\"\\Xtest.csv\", delimiter=\",\", skip_header=1))\n",
    "Ytest = torch.from_numpy(np.genfromtxt(path+\"\\Ytest.csv\", delimiter=\",\")).t()\n",
    "\n",
    "print(Xtrain.size(), Ytrain.size(), Xtest.size(), Ytest.size())\n",
    "Xtrain.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xtrain = Xtrain.type(torch.float32)\n",
    "#Ytrain = Ytrain.type(torch.float32)\n",
    "#Xtest = Xtest.type(torch.float32)\n",
    "#Ytest = Ytest.type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPRegression(gpytorch.models.ExactGP):\n",
    "    def __init__(self, Xtrain, Ytrain, likelihood):\n",
    "        super(GPRegression, self).__init__(Xtrain, Ytrain, likelihood)\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=2))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean = self.mean_module(x)\n",
    "        covar = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean, covar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = GPRegression(Xtrain, Ytrain, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for running the notebook in our testing framework\n",
    "import os\n",
    "smoke_test = ('CI' in os.environ)\n",
    "training_iter = 2 if smoke_test else 100\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikep\\anaconda3\\lib\\site-packages\\gpytorch\\lazy\\triangular_lazy_tensor.py:130: UserWarning: torch.triangular_solve is deprecated in favor of torch.linalg.solve_triangularand will be removed in a future PyTorch release.\n",
      "torch.linalg.solve_triangular has its arguments reversed and does not return a copy of one of the inputs.\n",
      "X = torch.triangular_solve(B, A).solution\n",
      "should be replaced with\n",
      "X = torch.linalg.solve_triangular(A, B). (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\BatchLinearAlgebra.cpp:1672.)\n",
      "  res = torch.triangular_solve(right_tensor, self.evaluate(), upper=self.upper).solution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/100 - Loss: 1520.038   lengthscales: 0.693, 0.693   noise: 0.693\n",
      "Iter 2/100 - Loss: 1415.505   lengthscales: 0.693, 0.693   noise: 0.744\n",
      "Iter 3/100 - Loss: 1320.631   lengthscales: 0.693, 0.693   noise: 0.798\n",
      "Iter 4/100 - Loss: 1234.616   lengthscales: 0.693, 0.693   noise: 0.854\n",
      "Iter 5/100 - Loss: 1156.691   lengthscales: 0.693, 0.693   noise: 0.911\n",
      "Iter 6/100 - Loss: 1086.125   lengthscales: 0.693, 0.693   noise: 0.971\n",
      "Iter 7/100 - Loss: 1022.231   lengthscales: 0.693, 0.693   noise: 1.031\n",
      "Iter 8/100 - Loss: 964.368   lengthscales: 0.693, 0.693   noise: 1.093\n",
      "Iter 9/100 - Loss: 911.942   lengthscales: 0.693, 0.693   noise: 1.156\n",
      "Iter 10/100 - Loss: 864.411   lengthscales: 0.693, 0.693   noise: 1.220\n",
      "Iter 11/100 - Loss: 821.278   lengthscales: 0.693, 0.693   noise: 1.284\n",
      "Iter 12/100 - Loss: 782.093   lengthscales: 0.693, 0.693   noise: 1.349\n",
      "Iter 13/100 - Loss: 746.450   lengthscales: 0.693, 0.693   noise: 1.413\n",
      "Iter 14/100 - Loss: 713.984   lengthscales: 0.693, 0.693   noise: 1.478\n",
      "Iter 15/100 - Loss: 684.366   lengthscales: 0.693, 0.693   noise: 1.542\n",
      "Iter 16/100 - Loss: 657.304   lengthscales: 0.693, 0.693   noise: 1.606\n",
      "Iter 17/100 - Loss: 632.535   lengthscales: 0.693, 0.693   noise: 1.669\n",
      "Iter 18/100 - Loss: 609.825   lengthscales: 0.693, 0.693   noise: 1.731\n",
      "Iter 19/100 - Loss: 588.967   lengthscales: 0.693, 0.693   noise: 1.793\n",
      "Iter 20/100 - Loss: 569.775   lengthscales: 0.693, 0.693   noise: 1.853\n",
      "Iter 21/100 - Loss: 552.082   lengthscales: 0.693, 0.693   noise: 1.913\n",
      "Iter 22/100 - Loss: 535.744   lengthscales: 0.693, 0.693   noise: 1.971\n",
      "Iter 23/100 - Loss: 520.627   lengthscales: 0.693, 0.693   noise: 2.029\n",
      "Iter 24/100 - Loss: 506.616   lengthscales: 0.693, 0.693   noise: 2.085\n",
      "Iter 25/100 - Loss: 493.606   lengthscales: 0.693, 0.693   noise: 2.140\n",
      "Iter 26/100 - Loss: 481.503   lengthscales: 0.693, 0.693   noise: 2.194\n",
      "Iter 27/100 - Loss: 470.225   lengthscales: 0.693, 0.693   noise: 2.247\n",
      "Iter 28/100 - Loss: 459.697   lengthscales: 0.693, 0.693   noise: 2.299\n",
      "Iter 29/100 - Loss: 449.852   lengthscales: 0.693, 0.693   noise: 2.350\n",
      "Iter 30/100 - Loss: 440.630   lengthscales: 0.693, 0.693   noise: 2.399\n",
      "Iter 31/100 - Loss: 431.977   lengthscales: 0.693, 0.693   noise: 2.447\n",
      "Iter 32/100 - Loss: 423.845   lengthscales: 0.693, 0.693   noise: 2.494\n",
      "Iter 33/100 - Loss: 416.191   lengthscales: 0.693, 0.693   noise: 2.541\n",
      "Iter 34/100 - Loss: 408.973   lengthscales: 0.693, 0.693   noise: 2.586\n",
      "Iter 35/100 - Loss: 402.158   lengthscales: 0.693, 0.693   noise: 2.630\n",
      "Iter 36/100 - Loss: 395.713   lengthscales: 0.693, 0.693   noise: 2.673\n",
      "Iter 37/100 - Loss: 389.608   lengthscales: 0.693, 0.693   noise: 2.715\n",
      "Iter 38/100 - Loss: 383.818   lengthscales: 0.693, 0.693   noise: 2.756\n",
      "Iter 39/100 - Loss: 378.318   lengthscales: 0.693, 0.693   noise: 2.796\n",
      "Iter 40/100 - Loss: 373.087   lengthscales: 0.693, 0.693   noise: 2.836\n",
      "Iter 41/100 - Loss: 368.104   lengthscales: 0.693, 0.693   noise: 2.874\n",
      "Iter 42/100 - Loss: 363.352   lengthscales: 0.693, 0.693   noise: 2.912\n",
      "Iter 43/100 - Loss: 358.814   lengthscales: 0.693, 0.693   noise: 2.949\n",
      "Iter 44/100 - Loss: 354.475   lengthscales: 0.693, 0.693   noise: 2.986\n",
      "Iter 45/100 - Loss: 350.320   lengthscales: 0.693, 0.693   noise: 3.021\n",
      "Iter 46/100 - Loss: 346.339   lengthscales: 0.693, 0.693   noise: 3.056\n",
      "Iter 47/100 - Loss: 342.518   lengthscales: 0.693, 0.693   noise: 3.091\n",
      "Iter 48/100 - Loss: 338.847   lengthscales: 0.693, 0.693   noise: 3.124\n",
      "Iter 49/100 - Loss: 335.317   lengthscales: 0.693, 0.693   noise: 3.157\n",
      "Iter 50/100 - Loss: 331.919   lengthscales: 0.693, 0.693   noise: 3.190\n",
      "Iter 51/100 - Loss: 328.644   lengthscales: 0.693, 0.693   noise: 3.222\n",
      "Iter 52/100 - Loss: 325.484   lengthscales: 0.693, 0.693   noise: 3.254\n",
      "Iter 53/100 - Loss: 322.434   lengthscales: 0.693, 0.693   noise: 3.285\n",
      "Iter 54/100 - Loss: 319.485   lengthscales: 0.693, 0.693   noise: 3.315\n",
      "Iter 55/100 - Loss: 316.633   lengthscales: 0.693, 0.693   noise: 3.345\n",
      "Iter 56/100 - Loss: 313.871   lengthscales: 0.693, 0.693   noise: 3.375\n",
      "Iter 57/100 - Loss: 311.195   lengthscales: 0.693, 0.693   noise: 3.404\n",
      "Iter 58/100 - Loss: 308.600   lengthscales: 0.693, 0.693   noise: 3.433\n",
      "Iter 59/100 - Loss: 306.082   lengthscales: 0.693, 0.693   noise: 3.461\n",
      "Iter 60/100 - Loss: 303.635   lengthscales: 0.693, 0.693   noise: 3.490\n",
      "Iter 61/100 - Loss: 301.258   lengthscales: 0.693, 0.693   noise: 3.517\n",
      "Iter 62/100 - Loss: 298.945   lengthscales: 0.693, 0.693   noise: 3.545\n",
      "Iter 63/100 - Loss: 296.695   lengthscales: 0.693, 0.693   noise: 3.572\n",
      "Iter 64/100 - Loss: 294.503   lengthscales: 0.693, 0.693   noise: 3.599\n",
      "Iter 65/100 - Loss: 292.367   lengthscales: 0.693, 0.693   noise: 3.625\n",
      "Iter 66/100 - Loss: 290.284   lengthscales: 0.693, 0.693   noise: 3.651\n",
      "Iter 67/100 - Loss: 288.253   lengthscales: 0.693, 0.693   noise: 3.677\n",
      "Iter 68/100 - Loss: 286.269   lengthscales: 0.693, 0.693   noise: 3.703\n",
      "Iter 69/100 - Loss: 284.332   lengthscales: 0.693, 0.693   noise: 3.728\n",
      "Iter 70/100 - Loss: 282.439   lengthscales: 0.693, 0.693   noise: 3.754\n",
      "Iter 71/100 - Loss: 280.589   lengthscales: 0.693, 0.693   noise: 3.779\n",
      "Iter 72/100 - Loss: 278.780   lengthscales: 0.693, 0.693   noise: 3.803\n",
      "Iter 73/100 - Loss: 277.009   lengthscales: 0.693, 0.693   noise: 3.828\n",
      "Iter 74/100 - Loss: 275.275   lengthscales: 0.693, 0.693   noise: 3.852\n",
      "Iter 75/100 - Loss: 273.578   lengthscales: 0.693, 0.693   noise: 3.876\n",
      "Iter 76/100 - Loss: 271.915   lengthscales: 0.693, 0.693   noise: 3.900\n",
      "Iter 77/100 - Loss: 270.285   lengthscales: 0.693, 0.693   noise: 3.924\n",
      "Iter 78/100 - Loss: 268.687   lengthscales: 0.693, 0.693   noise: 3.948\n",
      "Iter 79/100 - Loss: 267.120   lengthscales: 0.693, 0.693   noise: 3.971\n",
      "Iter 80/100 - Loss: 265.582   lengthscales: 0.693, 0.693   noise: 3.994\n",
      "Iter 81/100 - Loss: 264.073   lengthscales: 0.693, 0.693   noise: 4.017\n",
      "Iter 82/100 - Loss: 262.592   lengthscales: 0.693, 0.693   noise: 4.040\n",
      "Iter 83/100 - Loss: 261.138   lengthscales: 0.693, 0.693   noise: 4.063\n",
      "Iter 84/100 - Loss: 259.709   lengthscales: 0.693, 0.693   noise: 4.085\n",
      "Iter 85/100 - Loss: 258.305   lengthscales: 0.693, 0.693   noise: 4.108\n",
      "Iter 86/100 - Loss: 256.925   lengthscales: 0.693, 0.693   noise: 4.130\n",
      "Iter 87/100 - Loss: 255.569   lengthscales: 0.693, 0.693   noise: 4.152\n",
      "Iter 88/100 - Loss: 254.235   lengthscales: 0.693, 0.693   noise: 4.174\n",
      "Iter 89/100 - Loss: 252.923   lengthscales: 0.693, 0.693   noise: 4.196\n",
      "Iter 90/100 - Loss: 251.633   lengthscales: 0.693, 0.693   noise: 4.218\n",
      "Iter 91/100 - Loss: 250.363   lengthscales: 0.693, 0.693   noise: 4.239\n",
      "Iter 92/100 - Loss: 249.114   lengthscales: 0.693, 0.693   noise: 4.261\n",
      "Iter 93/100 - Loss: 247.883   lengthscales: 0.693, 0.693   noise: 4.282\n",
      "Iter 94/100 - Loss: 246.672   lengthscales: 0.693, 0.693   noise: 4.303\n",
      "Iter 95/100 - Loss: 245.479   lengthscales: 0.693, 0.693   noise: 4.325\n",
      "Iter 96/100 - Loss: 244.304   lengthscales: 0.693, 0.693   noise: 4.346\n",
      "Iter 97/100 - Loss: 243.147   lengthscales: 0.693, 0.693   noise: 4.366\n",
      "Iter 98/100 - Loss: 242.007   lengthscales: 0.693, 0.693   noise: 4.387\n",
      "Iter 99/100 - Loss: 240.883   lengthscales: 0.693, 0.693   noise: 4.408\n",
      "Iter 100/100 - Loss: 239.775   lengthscales: 0.693, 0.693   noise: 4.429\n"
     ]
    }
   ],
   "source": [
    "for i in range(training_iter):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(Xtrain)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, Ytrain)\n",
    "    loss.backward()\n",
    "    print(\"Iter %d/%d - Loss: %.3f   lengthscales: %.3f, %.3f   noise: %.3f\" % (\n",
    "        i + 1, training_iter, loss.item(),\n",
    "        model.covar_module.base_kernel.lengthscale.squeeze()[0],\n",
    "        model.covar_module.base_kernel.lengthscale.squeeze()[1],\n",
    "        model.likelihood.noise.item()\n",
    "    ))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get into evaluation (predictive posterior) mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Make predictions by feeding model through likelihood\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    observed_pred = likelihood(model(Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Xtrain.size(), Xtest.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting fit\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Initialize plot\n",
    "    f, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "    # Get upper and lower confidence bounds\n",
    "    lower, upper = observed_pred.confidence_region()\n",
    "    # Plot training data as black stars\n",
    "    ax.plot(Xtrain[:, 0].numpy(), Ytrain.numpy(), '*')\n",
    "    # Plot predictive means as blue line\n",
    "    ax.plot(Xtest[:, 0].numpy(), observed_pred.mean.numpy(), 'b')\n",
    "    # Shade between the lower and upper confidence bounds\n",
    "    ax.fill_between(Xtest[:, 0].numpy(), lower.numpy(), upper.numpy(), alpha=0.5)\n",
    "    #ax.set_ylim([-3, 3])\n",
    "    ax.legend(['Observed Data', 'Mean', 'Confidence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set into eval mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Initialize plots\n",
    "fig, ax = plt.subplots(2, 3, figsize=(14, 10))\n",
    "\n",
    "# Test points\n",
    "n1, n2 = 50, 50\n",
    "xv, yv = torch.meshgrid([torch.linspace(0, 1, n1), torch.linspace(0, 1, n2)])\n",
    "f, dfx, dfy = franke(xv, yv)\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad(), gpytorch.settings.fast_computations(log_prob=False, covar_root_decomposition=False):\n",
    "    test_x = torch.stack([xv.reshape(n1*n2, 1), yv.reshape(n1*n2, 1)], -1).squeeze(1)\n",
    "    predictions = likelihood(model(test_x))\n",
    "    mean = predictions.mean\n",
    "\n",
    "extent = (xv.min(), xv.max(), yv.max(), yv.min())\n",
    "ax[0, 0].imshow(f, extent=extent, cmap=cm.jet)\n",
    "ax[0, 0].set_title('True values')\n",
    "ax[0, 1].imshow(dfx, extent=extent, cmap=cm.jet)\n",
    "ax[0, 1].set_title('True x-derivatives')\n",
    "ax[0, 2].imshow(dfy, extent=extent, cmap=cm.jet)\n",
    "ax[0, 2].set_title('True y-derivatives')\n",
    "\n",
    "ax[1, 0].imshow(mean[:, 0].detach().numpy().reshape(n1, n2), extent=extent, cmap=cm.jet)\n",
    "ax[1, 0].set_title('Predicted values')\n",
    "ax[1, 1].imshow(mean[:, 1].detach().numpy().reshape(n1, n2), extent=extent, cmap=cm.jet)\n",
    "ax[1, 1].set_title('Predicted x-derivatives')\n",
    "ax[1, 2].imshow(mean[:, 2].detach().numpy().reshape(n1, n2), extent=extent, cmap=cm.jet)\n",
    "ax[1, 2].set_title('Predicted y-derivatives')\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
